{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23fe290",
   "metadata": {},
   "source": [
    "### ğŸ§  Step 1: Sliding Window Epoching (1s windows with 50% overlap)\n",
    "This replaces the older method of using one fixed-length chunk per trial. We now create multiple overlapping 1-second windows from the 10s trial to increase data samples and temporal resolution. This enhances model training diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "def sliding_window_epochs(raw, window_size_sec=1.0, overlap=0.5):\n",
    "    sfreq = int(raw.info['sfreq'])\n",
    "    window_size = int(window_size_sec * sfreq)\n",
    "    step_size = int(window_size * (1 - overlap))\n",
    "    data = raw.get_data()\n",
    "    n_samples = data.shape[1]\n",
    "    epochs = []\n",
    "    for start in range(0, n_samples - window_size + 1, step_size):\n",
    "        stop = start + window_size\n",
    "        epochs.append(data[:, start:stop])\n",
    "    return epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9ba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import mne\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Flatten, Dropout, BatchNormalization, Input,UpSampling1D\n",
    "from tensorflow.keras.layers import concatenate, Lambda, Conv2D, MaxPooling2D, GlobalAveragePooling2D,LSTM\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fea384",
   "metadata": {},
   "source": [
    "### ğŸ” Step 2: Time-Domain Data Augmentation\n",
    "This step introduces artificial variations to the training data using:\n",
    "- Random time shifts (jitter)\n",
    "- Gaussian noise\n",
    "- Random channel dropout\n",
    "\n",
    "This helps the model become more robust and prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def augment_batch(batch, jitter_max=6, noise_std=0.005, dropout_rate=0.1):\n",
    "    augmented = []\n",
    "    for trial in batch:\n",
    "        x = np.copy(trial)\n",
    "        # Time jitter (shift up to Â±6 samples)\n",
    "        shift = np.random.randint(-jitter_max, jitter_max + 1)\n",
    "        x = np.roll(x, shift, axis=1)\n",
    "        # Add Gaussian noise\n",
    "        x += noise_std * np.random.randn(*x.shape)\n",
    "        # Channel dropout\n",
    "        if dropout_rate > 0:\n",
    "            mask = np.random.rand(x.shape[0]) < dropout_rate\n",
    "            x[mask, :] = 0\n",
    "        augmented.append(x)\n",
    "    return np.array(augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d19547",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = 'Digit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9efc7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizearr = []\n",
    "X = np.zeros((230,14,1280))\n",
    "Y = np.zeros((230,))\n",
    "ctr = 0\n",
    "for fi in os.listdir(fo):\n",
    "    data = mne.io.read_raw_edf(os.path.join(fo,fi))\n",
    "    raw_data = data[2:16][0]*1000\n",
    "    raw_data = raw_data[:,0:1280]\n",
    "    #a = raw_data.shape\n",
    "    \n",
    "    _,cls = fi.split('_')\n",
    "    Y[ctr] = int(cls[0])\n",
    "    X[ctr,:,:] = raw_data\n",
    "    ctr = ctr+1\n",
    "\n",
    "    #sizearr.append(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.zeros((36110,32,14))\n",
    "Y_new = np.zeros((36110,))\n",
    "npt = 32\n",
    "stride = 8\n",
    "ctr = 0\n",
    "for i in range(0,230):\n",
    "    y = Y[i]\n",
    "    a= X[i,:,:]\n",
    "    a = a.transpose()\n",
    "    val = 0\n",
    "    while val<=(len(a)-npt):\n",
    "        x = a[val:val+npt,:]\n",
    "        X_new[ctr,:,:] = x\n",
    "        Y_new[ctr] = y\n",
    "        val = val+stride\n",
    "        ctr = ctr+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a62e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 2: Data Augmentation in the Time Domain\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def add_jitter(X, sigma=0.01):\n",
    "    return X + np.random.normal(loc=0., scale=sigma, size=X.shape)\n",
    "\n",
    "def window_slicing(X, window_ratio=0.9):\n",
    "    n_samples, time_len, channels = X.shape\n",
    "    new_time_len = int(time_len * window_ratio)\n",
    "    starts = np.random.randint(0, time_len - new_time_len + 1, size=n_samples)\n",
    "    X_sliced = np.array([x[start:start + new_time_len] for x, start in zip(X, starts)])\n",
    "    # Pad sliced windows back to original length\n",
    "    X_padded = np.zeros((n_samples, time_len, channels))\n",
    "    for i in range(n_samples):\n",
    "        X_padded[i, :new_time_len, :] = X_sliced[i]\n",
    "    return X_padded\n",
    "\n",
    "# Apply augmentation\n",
    "X_aug_jitter = add_jitter(X)\n",
    "X_aug_slice = window_slicing(X)\n",
    "\n",
    "# Combine original and augmented data\n",
    "X_combined = np.concatenate([X, X_aug_jitter, X_aug_slice], axis=0)\n",
    "y_combined = np.concatenate([y, y, y], axis=0)\n",
    "\n",
    "print(f\"Original shape: {X.shape}, Augmented shape: {X_combined.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2502407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_new, test_size=0.2, random_state=1)\n",
    "i1 = Input(shape=(32,14))\n",
    "x1 = BatchNormalization()(i1)\n",
    "x1 = Conv1D(128, kernel_size=10,strides=1,activation='relu',padding='same')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = MaxPooling1D(2)(x1)\n",
    "x1 = LSTM(256,activation='tanh')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dense(128, activation='relu')(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "output = Dense(10, activation='softmax')(x1)\n",
    "model = Model(inputs=i1, outputs=output)# summarize layers\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=10)\n",
    "model.fit(X_train, y=to_categorical(Y_train),validation_split=0.2,epochs=500, batch_size=128,verbose=1,callbacks=[es])\n",
    "pred = model.predict(X_test)\n",
    "Y_pred = np.argmax(pred,axis=1)\n",
    "print(accuracy_score(Y_pred,Y_test))\n",
    "sns.heatmap(confusion_matrix(Y_test,Y_pred), annot=True,fmt='g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
